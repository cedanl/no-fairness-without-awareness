---
title: "Transparante Kansengelijkheidsanalyse"
subtitle: "`r params$subtitle`"
params:
  subtitle: "Opleiding Onbekend"
  temp_dir: "temp"
  work_dir: "."
format:
  pdf:
    documentclass: article
    geometry: margin=0.75in
    fontsize: 11pt
    papersize: A4
    toc: false
    toc-depth: 2
    number-sections: false
    keep-tex: false
    include-in-header:
      text: |
        \usepackage{setspace}
        \setstretch{1.05}
        \usepackage{parskip}
        \setlength{\parskip}{0.6em}
        \usepackage{titlesec}
        \usepackage{xcolor}
        % Section formatting
        \titleformat{\section}
          {\Large\bfseries\color[gray]{0.2}}
          {\thesection}{1em}{}
          [\vspace{0.3em}\hrule\vspace{0.3em}]
        \titlespacing*{\section}{0pt}{1.2em}{0.8em}
        % Subsection formatting
        \titleformat{\subsection}
          {\large\bfseries}
          {\thesubsection}{0.8em}{}
        \titlespacing*{\subsection}{0pt}{0.9em}{0.5em}
        % Subsubsection formatting
        \titleformat{\subsubsection}
          {\normalsize\bfseries}
          {\thesubsubsection}{0.6em}{}
        \titlespacing*{\subsubsection}{0pt}{0.6em}{0.3em}
execute:
  cache: false
---

```{r}
#| label: setup
#| echo: false
#| include: false

# Load nfwa package or source files in development
library(nfwa)

# Set knitr's root directory to the user's working directory
# This ensures all paths are resolved relative to where the user is working
knitr::opts_knit$set(root.dir = params$work_dir)
```

# Hoofdbevindingen

```{r}
#| echo: false
#| results: asis

# Load conclusions
temp_path <- params$temp_dir
conclusions_list <- readRDS(file.path(temp_path, "conclusions_list.rds"))

# Create executive summary
has_bias <- any(sapply(conclusions_list, function(x) !grepl("geen sprake van bias", tolower(x))))

if (has_bias) {
  cat("**Er is bias geconstateerd** in één of meer onderzochte variabelen.\n\n")
  cat("Dit rapport identificeert verschillen in studiesucces tussen groepen studenten op basis van persoonskenmerken. Deze bevindingen vereisen nadere analyse en mogelijke interventies om kansengelijkheid te bevorderen.\n")
} else {
  cat("**Geen significante bias geconstateerd** in de onderzochte variabelen.\n\n")
  cat("De analyse toont geen substantiële verschillen in studiesucces tussen groepen op basis van de onderzochte persoonskenmerken.\n")
}
```

## Kansengelijkheid per variabele

```{r}
#| echo: false
#| results: asis

temp_dir <- params$temp_dir

for (var_name in names(conclusions_list)) {
  var_title <- stringr::str_to_title(var_name)
  cat("**", var_title, "**", ":", sep = "")
  cat(conclusions_list[[var_name]], "\n\n")
}
```

\newpage 

# Inleiding

Het lectoraat Learning Technology & Analytics (LTA) van De Haagse Hogeschool heeft tot doel kansengelijkheid voor studenten te verhogen met behulp van learning analytics en inzet van learning technology.

Het lectoraat heeft een onderzoeksmethode ontwikkeld om te kunnen analyseren of er sprake is van bias in studiedata in relatie tot het succes van studenten, wat een indicatie kan zijn van een gebrek aan kansengelijkheid. Deze methode gebruikt prognosemodellen op basis van machine learning. Een prognosemodel is dus niet een doel op zich, maar het instrument voor een analyse van kansengelijkheid, ook wel een *fairness* analyse genoemd.

Over deze methode heeft de lector, Dr. Theo Bakker, zijn intreerede uitgesproken op 21 november 2024, getiteld: '[No Fairness without Awareness. Toegepast onderzoek naar kansengelijkheid in het hoger onderwijs. Intreerede lectoraat Learning Technology & Analytics.](https://zenodo.org/records/14204674)' [@Bakker.2024-intreerede].

Zie voor een verdere toelichting op het gehele onderzoeksprogramma: '[No Fairness without Awareness](https://www.dehaagsehogeschool.nl/onderzoek/kenniscentra/no-fairness-without-awareness)'.

## Wat is een kansengelijkheidsanalyse?

Kansengelijkheid betekent dat alle studenten gelijke kansen hebben om succesvol te zijn, ongeacht hun achtergrond, geslacht, vooropleiding of andere kenmerken. In dit rapport onderzoeken we of ons voorspellingsmodel, dat schat hoe waarschijnlijk het is dat een student naar het volgende jaar doorgaat, eerlijk werkt voor alle groepen studenten.

**Bias** ontstaat wanneer een systeem of model systematisch minder goed werkt voor bepaalde groepen. Dit kan onopzettelijk gebeuren, zelfs wanneer iedereen het beste voorheeft. Denk aan een arts die verschillende diagnoses stelt op basis van dezelfde symptomen, afhankelijk van het geslacht van de patiënt, dat zou medisch bezien oneerlijk zijn. Net zo onderzoeken we of ons model eerlijk is.

## Hoe werkt deze analyse?

Deze analyse bestaat uit drie stappen:

1. **We verzamelen historische data**: We kijken naar gegevens van studenten uit voorgaande jaren (geslacht, vooropleiding, behaalde cijfers, en of ze met succes naar het volgende jaar doorgingen).

2. **Een computerprogramma leert patronen**: We trainen een machine learning model (een soort statistische leermeester) die leert welke factoren samenhangen met studiesucces. Het model leert patronen uit de data.

3. **We controleren eerlijkheid**: We vergelijken hoe goed het model voorspelt voor verschillende groepen studenten. Werkt het even goed voor mannen en vrouwen? Voor studenten met verschillende vooropleidingen? Of voorspelt het model voor sommige groepen beter of slechter?

Dit proces is vergelijkbaar met het testen van een thermometer: je wilt niet alleen weten of deze nauwkeurig is, maar ook of deze even nauwkeurig temperatuur meet voor iedereen.

\newpage

# Methodologie

De NFWA methode gebruikt machine learning als diagnostisch instrument om bias in studiedata te identificeren. We controleren of de voorspellingen van het model systematisch verschillen tussen groepen op basis van gevoelige kenmerken (geslacht, vooropleiding, aansluiting). Dit helpt ons bij het begrijpen van potentiële ongelijkheden in onderwijs en hoe we die kunnen aanpakken.

## Analysestappen

### Stap 1: Data voorbereiding en kenmerken

```{r}
#| echo: false
#| results: asis

# Load metadata if available
metadata_file <- file.path(temp_path, "analysis_metadata.rds")
if (file.exists(metadata_file)) {
  metadata <- readRDS(metadata_file)

  cat("- **Aantal studenten**: ", metadata$n_students, "\n")
  cat("- **Onderzochte uitkomst**: Retentie, ofwel 'doorgaan naar jaar 2'\n")
  cat("  - Dit betekent: Is de student succesvol geweest in het eerste jaar en gaat door naar het tweede jaar?\n")
  cat("- **Gevoelige variabelen**: ", paste(names(conclusions_list), collapse = ", "), "\n")
  cat("  - Dit zijn kenmerken waarvoor we **geen kansenongelijkheid willen** (geen ongelijke behandeling gebaseerd op deze factoren)\n\n")
} else {
  cat("- **Onderzochte uitkomst**: Retentie, ofwel 'doorgaan naar jaar 2'\n")
  cat("  - Dit betekent: Is de student succesvol geweest in het eerste jaar en gaat door naar het tweede jaar?\n")
  cat("- **Gevoelige variabelen**: De kenmerken waarvoor we **geen kansenongelijkheid willen**\n\n")
}
```

### Stap 2: Model training (het leren van patronen)

```{r}
#| echo: false
#| results: asis

metadata_file <- file.path(temp_path, "analysis_metadata.rds")
if (file.exists(metadata_file)) {
  metadata <- readRDS(metadata_file)

  cat("We trainen twee verschillende typen modellen:\n\n")
  cat("**1. Logistische Regressie (met LASSO regularisatie)**\n\n")
  cat("- Dit is een statistische methode die naar verbanden kijkt. Het zoekt uit welke factoren het meest belangrijk zijn voor studiesucces.\n")
  cat("- LASSO betekent dat het alleen de belangrijkste factoren behoudt (rest verwijdert)\n\n")
  cat("**2. Random Forest (1000 kleine beslisbomen)**\n\n")
  cat("- Dit model werkt als volgt: Het combineert veel kleine 'Ja/Nee'-vragen.\n")
  cat("- Iedere boom maakt zijn eigen voorspelling, dan wordt de meerderheid gekozen.\n")
  cat("- Het is vergelijkbaar met een team van experts die gezamenlijk een diagnose stellen.\n\n")
  cat("**Waarom twee modellen?** Om zeker te weten welke het meest betrouwbaar is voor uw studenten.\n\n")
  cat("**Gekozen model voor deze analyse**: ", metadata$best_model, "\n\n")
  cat("**Model prestatie (AUC)**: ", round(metadata$model_auc, 3), "\n\n")
  cat("- AUC loopt van 0 tot 1 (hoger = beter)\n")
  cat("- 0.5 = volledig gokken\n")
  cat("- \\> 0.7 = acceptabel prestatief\n")
  cat("- 0.8+ = goed prestatief\n\n")
} else {
  cat("We trainen twee verschillende modellen en selecteren het beste:\n\n")
  cat("- **Logistische Regressie**: Statistische methode, makkelijk uit te leggen\n")
  cat("- **Random Forest**: Machine learning methode, vaak nauwkeuriger\n")
  cat("Daarna kiezen we het meest betrouwbare model.\n\n")
}
```

### Stap 3: Fairness evaluatie (controleren op eerlijkheid)

**Wat is de referentiegroep?**

Voor elke analyse wordt één groep als referentie (vergelijkingspunt) gekozen. De referentiegroep is **gekozen als de grootste groep** binnen die variabele. Bijvoorbeeld: als er meer mannen dan vrouwen zijn, worden mannen de referentiegroep.

**Waarom een referentiegroep?** Om bias te kunnen meten, hebben we een vergelijkingspunt nodig. We kunnen niet zeggen of een model "eerlijk" is in absolute zin. We kunnen alleen zeggen of het even goed werkt voor verschillende groepen. Daarom vergelijken we alle andere groepen met de referentiegroep om te zien of het model even goed werkt.

**Let op:** In de resultaten tabel wordt de referentiegroep weergegeven als "NTB" (Nader Te Bepalen), omdat deze groep zelf niet op bias kan worden beoordeeld. Het is immers het vergelijkingspunt zelf.

Voor elke gevoelige variabele meten we vier fairness-metrieken. Dit zijn manieren om te controleren of ons model eerlijk werkt:

-   **Equal Opportunity**:
    - Vraag: "Geeft het model even veel goede voorspellingen voor succesvolle studenten uit alle groepen?"
    - Voorbeeld: Voorspelt het model even goed voor mannen als voor vrouwen die daadwerkelijk doorgaan naar jaar 2?

-   **Predictive Parity**:
    - Vraag: "Zijn de voorspellingen even betrouwbaar voor alle groepen?"
    - Voorbeeld: Klopt het voorspeld succes even goed voor alle vooropleidingen?

-   **Accuracy Equality**:
    - Vraag: "Is het model even nauwkeurig voor iedereen?"
    - Voorbeeld: Maakt het model evenveel fouten bij alle groepen?

-   **Statistical Parity**:
    - Vraag: "Krijgen alle groepen gelijk veel positieve voorspellingen?"
    - Voorbeeld: Voorspelt het model succes voor dezelfde procentuele hoeveelheid van iedere groep?

### Stap 4: Bias classificatie (beoordeling)

Na het meten van deze eerlijkheidsmetrieken classificeren we met een score (FRN):

-   **Negatieve bias** (FRN score < 0.8):
    - Het model presteert slechter voor deze groep
    - Bijvoorbeeld: het model voorspelt succes minder goed voor vrouwen
    - Dit kan duiden op kansenongelijkheid in de data of het model

-   **Positieve bias** (FRN score > 1.25):
    - Het model presteert beter voor deze groep
    - Bijvoorbeeld: het model voorspelt succes beter voor mannen
    - Dit kan ook kansenongelijkheid zijn.

-   **Geen bias** (FRN score tussen 0.8 en 1.25):
    - Het model werkt redelijk eerlijk voor deze groep
    - Geen significante kansenongelijkheid gedetecteerd
    
### Belangrijke overwegingen: Minimale groepsgrootte

We hanteren een **minimum van 15 studenten per categorie** om betrouwbare conclusies te kunnen trekken:

-   Met minder dan 15 studenten is het statistisch onbetrouwbaar (net zoals een enquête onder slechts 5 mensen niet representatief is)
-   Kleinere groepen worden wel in het rapport opgenomen, maar krijgen het label "Data laag".
-   Dit betekent niet dat er geen bias is, maar dat we het niet zeker kunnen zeggen vanwege te weinig data


\newpage

# Resultaten

```{r}
#| echo: false
#| fig-align: center
#| out.width: "85%"
#| fig-height: 3.5

# Since root.dir is set to work_dir, we can use the relative path
img_path <- file.path(params$temp_dir, "result_table.png")
knitr::include_graphics(img_path)
```

**Legenda:** N = aantal studenten \| % = percentage \| Bias = Ja/Nee/NTB (Nader Te Bepalen)

### Kleurcodes

$\providecolor{sqfill1}{HTML}{A84268}\providecolor{sqborder1}{HTML}{A9A9A9}\begingroup\setlength{\fboxsep}{1pt}\fcolorbox{sqborder1}{sqfill1}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\endgroup$ Negatieve bias \| $\providecolor{sqfill2}{HTML}{9DBF9E}\providecolor{sqborder2}{HTML}{A9A9A9}\begingroup\setlength{\fboxsep}{1pt}\fcolorbox{sqborder2}{sqfill2}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\endgroup$ Positieve bias \| $\providecolor{sqfill3}{HTML}{FCB97D}\providecolor{sqborder3}{HTML}{A9A9A9}\begingroup\setlength{\fboxsep}{1pt}\fcolorbox{sqborder3}{sqfill3}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\endgroup$ Data laag \| $\providecolor{sqfill4}{HTML}{E5E5E5}\providecolor{sqborder4}{HTML}{A9A9A9}\begingroup\setlength{\fboxsep}{1pt}\fcolorbox{sqborder4}{sqfill4}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\endgroup$ Referentie

\newpage

## Bijlage A: Gedetailleerde visualisaties

Deze bijlage bevat de fairness-metrieken en model-voorspellingsplots voor elke onderzochte variabele.


1.  **Fairness-metrieken (staafdiagram)**

Dit diagram vergelijkt hoe het model presteert voor verschillende groepen:

-   **Referentiegroep**: De grootste groep binnen de variabele (bijv. mannen bij geslacht). Deze groep staat op de verticale zwarte lijn bij 0.0.

-   **X-as**: Verschil ten opzichte van de referentiegroep

    -   Staven naar links (negatief) = model presteert slechter dan voor referentiegroep

    -   Staven naar rechts (positief) = model presteert beter dan voor referentiegroep

    -   Op de zwarte lijn (0.0) = gelijk aan referentiegroep

-   **Y-as**: Vijf verschillende fairness-metrieken

-   **Gekleurde zones**: Rood = negatieve bias, Groen = positieve bias, Wit = acceptabel bereik

-   Hoe verder de staaf van 0.0 afligt, hoe groter het verschil met de referentiegroep

2.  **Model-voorspellingen (dichtheidsgrafiek)**

Dit diagram toont hoe het model voorspellingen maakt per groep:

-   **X-as**: Voorspelde kans op studiesucces (0-100%)

-   **Y-as**: Dichtheid (hoe veel studenten bij die voorspelling)

-   Wanneer krommen sterk uit elkaar liggen = model behandelt groepen verschillend

-   Overlappende krommen = eerlijk model

-   Verschuiving naar links = lagere voorspelde kansen

\newpage

```{r}
#| echo: false
#| results: asis

temp_dir <- params$temp_dir
conclusions_list <- readRDS(file.path(temp_dir, "conclusions_list.rds"))

for (var_name in names(conclusions_list)) {
  var_title <- stringr::str_to_title(var_name)
  cat("\n## ", var_title, "\n\n")

  fairness_file <- file.path(temp_dir, paste0("fairness_plot_", var_name, ".png"))
  density_file <- file.path(temp_dir, paste0("fairness_density_", var_name, ".png"))

  if (file.exists(fairness_file)) {
    cat("![](", fairness_file, "){ width=90% }\n\n")
  }
  if (file.exists(density_file)) {
    cat("![](", density_file, "){ width=90% }\n\n")
  }
  cat("\\newpage\n\n")
}
```

\newpage

## Bijlage B: Glossarium

| Term | Omschrijving |
|---|---|
| **AUC** | Area Under the Curve - model prestatie (1.0 = perfect, 0.5 = gokken, \>0.7 = acceptabel) |
| **Bias** | Systematische verschillen in model-voorspellingen tussen groepen; wanneer een systeem bepaalde groepen benadeelt |
| **Equal Opportunity** | Model voorspelt even goed voor succesvolle studenten uit alle groepen |
| **Predictive Parity** | Positieve voorspellingen zijn even betrouwbaar over alle groepen |
| **Accuracy Equality** | Algemene nauwkeurigheid model gelijk over alle groepen |
| **Statistical Parity** | Proportie positieve voorspellingen gelijk over alle groepen |
| **FRN Score** | Fairness Ratio Number - eerlijkheidsmaatstaf: 1.0 = gelijk, \<0.8 = negatieve bias, \>1.25 = positieve bias |
| **Retentie** | Student gaat door naar tweede studiejaar; maatstaf van studiesucces |
| **Sensitieve variabele** | Persoonskenmerken waarvoor kansenongelijkheid ongewenst is (geslacht, vooropleiding, etniciteit, etc.) |
| **Machine Learning** | Computertechniek waarbij systemen automatisch patronen leren uit data zonder expliciet geprogrammeerd te worden |
| **Model** | Een wiskundige representatie van patronen in data; een "recept" om voorspellingen te maken |
| **Training** | Het proces waarbij een machine learning model leert van historische data |
| **Voorspelling** | Een schatting van toekomstige uitkomsten gebaseerd op het getrainde model |
| **Logistische Regressie** | Een statistische methode die binaire uitkomsten voorspelt (ja/nee); makkelijk uit te leggen |
| **Random Forest** | Een machine learning methode die veel kleine beslisbomen combineert; meestal nauwkeuriger maar moeilijker uit te leggen |
| **LASSO Regularisatie** | Een techniek die alleen de belangrijkste factoren behoudt en onbelangrijke verwijdert |
| **Fairness** | Kansengelijkheid; het principe dat systemen iedereen eerlijk behandelen ongeacht hun achtergrond |
| **Interpretabiliteit** | Hoe goed we kunnen begrijpen waarom een model bepaalde voorspellingen doet |
| **Cohort** | Een groep studenten die in dezelfde periode aan een opleiding zijn begonnen |
| **N** | Aantal; in dit rapport: aantal studenten in een groep |

## Bijlage C: Referenties en bronnen


**Wetenschappelijke basis:**

Bakker, T. (2024). *No Fairness without Awareness. Toegepast onderzoek naar kansengelijkheid in het hoger onderwijs.* Intreerede lectoraat Learning Technology & Analytics. https://doi.org/10.5281/zenodo.14204674

**Meer informatie:**

-   Lectoraat Learning Technology & Analytics: https://www.dehaagsehogeschool.nl/onderzoek/kenniscentra/no-fairness-without-awareness
-   NFWA R package repository: https://github.com/cedanl/no-fairness-without-awareness

**Contact:**

Voor vragen over deze analyse kunt u contact opnemen met:

Lectoraat Learning Technology & Analytics De Haagse Hogeschool Web: http://www.hhs.nl


## Bijlage D: Technische details


```{r}
#| echo: false
#| results: asis

# Load and display technical metadata if available
metadata_file <- file.path(temp_path, "analysis_metadata.rds")
if (file.exists(metadata_file)) {
  metadata <- readRDS(metadata_file)

  cat("**Analyse-parameters:**\n\n")
  cat("- Datum analyse:", format(Sys.Date(), "%d-%m-%Y"), "\n")
  cat("- Aantal studenten:", metadata$n_students, "\n")
  cat("- Gekozen model:", metadata$best_model, "\n")
  cat("- Model AUC:", round(metadata$model_auc, 4), "\n")
  cat("- Fairness cutoff (FRN):", metadata$cutoff, "\n")
  cat("- Onderzochte variabelen:", paste(names(conclusions_list), collapse = ", "), "\n")
  cat("- R package versie: nfwa", as.character(packageVersion("nfwa")), "\n")

  # Display EOI if available from metadata
  if (!is.null(metadata$eoi)) {
    cat("- Eerste jaar aan deze opleiding/instelling (EOI):", metadata$eoi, "\n")
  }
} else {
  cat("**Analyse-parameters:**\n\n")
  cat("- Datum analyse:", format(Sys.Date(), "%d-%m-%Y"), "\n")
  cat("- Onderzochte variabelen:", paste(names(conclusions_list), collapse = ", "), "\n")
  cat("- R package versie: nfwa", as.character(packageVersion("nfwa")), "\n")
}
```
